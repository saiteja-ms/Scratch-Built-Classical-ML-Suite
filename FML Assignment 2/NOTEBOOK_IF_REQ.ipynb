{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FhiO3Mz7rYZ0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data for stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize stop words and stemmer\n",
        "stop_words = set(stopwords.words('english'))  # Set of English stop words\n",
        "stemmer = PorterStemmer()  # Initialize the Porter Stemmer for stemming words\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and non-alphabetic characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Tokenize, remove stop words, and stem the words\n",
        "    tokens = [stemmer.stem(word) for word in text.split() if word not in stop_words and len(word) > 2]\n",
        "    return ' '.join(tokens)  # Join tokens back into a single string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCdc1jbHrfxh",
        "outputId": "271d62fa-505c-438b-88f4-399b0c8b33f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess emails from a folder\n",
        "def load_and_preprocess_emails(folder):\n",
        "    emails = []  # Initialize an empty list to store emails\n",
        "    for filename in os.listdir(folder):\n",
        "        file_path = os.path.join(folder, filename)  # Get full path for the email file\n",
        "        try:\n",
        "            # Attempt to read the email with UTF-8 encoding\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                text = file.read()  # Read the content of the email\n",
        "                processed_text = preprocess_text(text)  # Preprocess the email text\n",
        "                emails.append(processed_text)  # Append processed text to the list\n",
        "        except UnicodeDecodeError:\n",
        "            # Handle potential encoding issues by trying a different encoding\n",
        "            with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
        "                text = file.read()\n",
        "                processed_text = preprocess_text(text)\n",
        "                emails.append(processed_text)\n",
        "    return emails  # Return the list of processed emails"
      ],
      "metadata": {
        "id": "m1jsOB0irj9Z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the contents of the zip file\n",
        "zip_file_path = '/content/FML_DATA.zip'\n",
        "extracted_folder = '/content/extracted_data'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder)  # Extract all files to the specified folder"
      ],
      "metadata": {
        "id": "ZlwkmtwdrnJA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths for spam and ham folders\n",
        "spam_folder = os.path.join(extracted_folder, 'spam')\n",
        "ham_folder = os.path.join(extracted_folder, 'ham')\n",
        "\n",
        "# Load emails from both spam and ham folders\n",
        "spam_emails = load_and_preprocess_emails(spam_folder)  # Load spam emails\n",
        "ham_emails = load_and_preprocess_emails(ham_folder)    # Load ham emails"
      ],
      "metadata": {
        "id": "UUyx_r0H5M8U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction with TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=1000)  # Initialize TF-IDF vectorizer\n",
        "all_emails = spam_emails + ham_emails  # Combine spam and ham emails\n",
        "X = vectorizer.fit_transform(all_emails)  # Fit and transform the combined emails into TF-IDF features"
      ],
      "metadata": {
        "id": "DX5Ha8Em5M5V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels: 1 for spam, 0 for ham\n",
        "y = [1] * len(spam_emails) + [0] * len(ham_emails)\n",
        "\n",
        "# Split the dataset into training and validation sets, stratifying by labels to maintain balance\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "rWtwFLd-5M2U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We are going to train the following models on our training dataset and validate them on our validation dataset:\n",
        "##1. K-Nearest Neighbours(KNN)\n",
        "##2. Naive Bayes Classifier\n",
        "##3. Logistic Regression\n",
        "##4. Support Vector Machine(SVM)\n",
        "##5. Perceptron Algorithm"
      ],
      "metadata": {
        "id": "jL6nUugNDhsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Building K-Nearest Neighbours from scratch"
      ],
      "metadata": {
        "id": "OU4YGlxVDwVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = [self._predict(x) for x in X]\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        distances = np.linalg.norm(self.X_train - x, axis=1)\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "        most_common = np.bincount(k_nearest_labels).argmax()\n",
        "        return most_common\n"
      ],
      "metadata": {
        "id": "pKR7W_275MxB"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We take k to be 5\n",
        "k = 5 # We can do cross-validation, but considering the computational time required, we will go with common k value\n",
        "knn = KNN(k=k)\n",
        "knn.fit(X_train.toarray(), y_train)\n",
        "predictions = knn.predict(X_val.toarray())\n",
        "accuracy = accuracy_score(y_val, predictions)\n"
      ],
      "metadata": {
        "id": "abD1KjzM5Mts"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16Lp2U70IJHb",
        "outputId": "3c2a54e8-0521-40aa-d219-9c808c284b6e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9619686800894854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN model has achieved an accuracy score of 96.2% on its validation set."
      ],
      "metadata": {
        "id": "k_32PX2ede__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Building Naive-Bayes Classifier"
      ],
      "metadata": {
        "id": "g_n_M6HIIMqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        self.parameters = {}\n",
        "        for c in self.classes:\n",
        "            X_c = X[y==c]\n",
        "            self.parameters[c] = {\n",
        "                'mean': X_c.mean(axis=0),\n",
        "                'var': X_c.var(axis=0),\n",
        "                'prior': len(X_c) / len(X)\n",
        "            }\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._predict(x) for x in X]\n",
        "\n",
        "    def _predict(self, x):\n",
        "        posteriors = []\n",
        "        for c in self.classes:\n",
        "            prior = self.parameters[c]['prior']\n",
        "            # Calculate likelihood for the entire document\n",
        "            likelihood = np.prod(self._pdf(c, x))\n",
        "            posterior = likelihood * prior\n",
        "            posteriors.append(posterior)\n",
        "        return self.classes[np.argmax(posteriors)]\n",
        "\n",
        "    def _pdf(self, c, x):\n",
        "        mean = self.parameters[c]['mean']\n",
        "        var = self.parameters[c]['var']\n",
        "        numerator = np.exp(-(x-mean)**2/(2*var))\n",
        "        denominator = np.sqrt(2*np.pi*var)\n",
        "        return numerator / denominator"
      ],
      "metadata": {
        "id": "9loBFff9-EdN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes = NaiveBayes()\n",
        "naive_bayes.fit(X_train.toarray(), y_train)"
      ],
      "metadata": {
        "id": "A1hpDRkIBe9l"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_nb = naive_bayes.predict(X_val.toarray())\n",
        "accuracy_score(prediction_nb, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaMWPalpBhi0",
        "outputId": "e077720d-dfb1-4b19-f60b-0fa2b98939b1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-c8f75266acb2>:29: RuntimeWarning: invalid value encountered in divide\n",
            "  numerator = np.exp(-(x-mean)**2/(2*var))\n",
            "<ipython-input-12-c8f75266acb2>:29: RuntimeWarning: divide by zero encountered in divide\n",
            "  numerator = np.exp(-(x-mean)**2/(2*var))\n",
            "<ipython-input-12-c8f75266acb2>:31: RuntimeWarning: invalid value encountered in divide\n",
            "  return numerator / denominator\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46308724832214765"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes classifier has achieved an accuracy score of 46.3% on its validation set."
      ],
      "metadata": {
        "id": "77o1au-1dXj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Building Logistic Regression Model from scratch"
      ],
      "metadata": {
        "id": "96DuTBA7cxIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_predicted = self._sigmoid(linear_model)\n",
        "\n",
        "            # Update weights and bias\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self._sigmoid(linear_model)\n",
        "        return [1 if i>0.5 else 0 for i in y_predicted]\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "yAnnQ88_EtuZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Logistic Regression model\n",
        "lr = LogisticRegression(learning_rate=0.1, n_iters=1000)\n",
        "lr.fit(X_train.toarray(), y_train) # Convert X_train to dense array"
      ],
      "metadata": {
        "id": "yPl4jy6_HBFr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "y_pred = lr.predict(X_val.toarray())"
      ],
      "metadata": {
        "id": "XGdhs8LwHD1B"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SQM6k2IHGxT",
        "outputId": "980fe468-b77f-4445-8e67-0e104a05e975"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression model has achieved an accuracy score of 89.4% on its validation set."
      ],
      "metadata": {
        "id": "1wsXd1codHiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Building SVM from scratch"
      ],
      "metadata": {
        "id": "Syxqr5xDdnvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "class SupportVectorMachine:\n",
        "    def __init__(self, kernel='linear', C=1):\n",
        "        self.kernel = kernel\n",
        "        self.C = C\n",
        "        self.model = svm.SVC(kernel=kernel, C=C)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n"
      ],
      "metadata": {
        "id": "I4lT6J8JHMyy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "for k in kernel:\n",
        "  svm_model = SupportVectorMachine(kernel=k, C=1)\n",
        "  svm_model.fit(X_train.toarray(), y_train)\n",
        "  # Make predictions\n",
        "  y_pred = svm_model.predict(X_val.toarray())\n",
        "  # Evaluate model\n",
        "  accuracy = accuracy_score(y_val, y_pred)\n",
        "  print(f\"Accuracy: {accuracy:.3f}for SVM having kernel {k}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROdiFQNOHVHx",
        "outputId": "dd369fa6-a6da-4fcd-d81a-9d4a06e3d5f8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.974for SVM having kernel linear\n",
            "Accuracy: 0.967for SVM having kernel poly\n",
            "Accuracy: 0.979for SVM having kernel rbf\n",
            "Accuracy: 0.970for SVM having kernel sigmoid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We can observe that SVM having rbf kernel gives the maximum accuracy of 97.9% on its validation set."
      ],
      "metadata": {
        "id": "em6CWvdmTDNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_rbf = SupportVectorMachine(kernel='rbf', C=1)\n",
        "svm_rbf.fit(X_train.toarray(), y_train)"
      ],
      "metadata": {
        "id": "MqFHUbgYa1N9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Building Perceptron Algorithm from scratch"
      ],
      "metadata": {
        "id": "PYTkihM9dTjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, lr=0.01, n_iters=250):\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.wts = None\n",
        "        self.bias = None\n",
        "        self.activation_fn = self.activation_func\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.wts = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                #If x_i is sparse, convert it to dense before dot product\n",
        "                if isinstance(x_i,(np.matrix, np.ndarray)):\n",
        "                  x_i = x_i.toarray()[0] if x_i.shape[0] == 1 else x_i\n",
        "\n",
        "                linear_output = np.dot(x_i, self.wts) + self.bias\n",
        "                # The linear_output is already a scalar or a dense array, no need to call toarray()\n",
        "                y_predicted = self.activation_func(linear_output)\n",
        "\n",
        "                #if x_i is sparse, convert it to dense for update\n",
        "                if isinstance(x_i,(np.matrix, np.ndarray)):\n",
        "                  x_i = x_i.toarray()[0] if x_i.shape[0] == 1 else x_i\n",
        "\n",
        "                update = self.lr * (y[idx] - y_predicted)\n",
        "                self.wts += update * x_i\n",
        "                self.bias += update\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Convert X to dense array before prediction if it's sparse\n",
        "        if isinstance(X, (scipy.sparse.csr_matrix, scipy.sparse.csc_matrix)):\n",
        "            X = X.toarray()  # Convert to dense array if sparse\n",
        "\n",
        "        linear_output = np.dot(X, self.wts) + self.bias\n",
        "        y_pred = self.activation_fn(linear_output)\n",
        "        return y_pred\n",
        "\n",
        "    def activation_func(self, x):\n",
        "        return np.where(x >= 0, 1, 0)"
      ],
      "metadata": {
        "id": "2vKUVvioId6C"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percept = Perceptron(lr=0.5, n_iters=250)\n",
        "percept.fit(X_train.toarray(), y_train)\n",
        "\n",
        "y_pred = percept.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Accuracy: {accuracy:} for perceptron algorithm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xotKj7dV2Sw",
        "outputId": "bd3c50f8-66d0-46d3-aa59-bdeb4f4613ea"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9642058165548099 for perceptron algorithm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptron algorithm is giving an accuracy of around 96.4%"
      ],
      "metadata": {
        "id": "Wt7AVBi1YDCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence, here is the compilation of accuracies of all the algorithms that we have trained and validated till now:\n",
        "1. K-Nearest Neighbours - 96.2\n",
        "2. Naive Bayes Classifier - 46.3%\n",
        "2. Logistic Regression  - 89.4\n",
        "3. SVM ( RBF Kernel)    - 97.9\n",
        "4. Perceptron           - 96.4\n",
        "\n",
        "So, we will build our final working fnction using SVM's RBF kernel, to read mails in \"test\" folder, and make predictions accordingly as \"spam\" or \"ham(non-spam)"
      ],
      "metadata": {
        "id": "3TOlolzIYI01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def predict_test_emails(svm_model, vectorizer):\n",
        "    test_folder = 'test' # Modify the 'test' folder address\n",
        "    test_emails = []\n",
        "\n",
        "    # Read emails from test folder\n",
        "    for filename in os.listdir(test_folder):\n",
        "        if filename.startswith('email') and filename.endswith('.txt'):\n",
        "            file_path = os.path.join(test_folder, filename)\n",
        "            try:\n",
        "                # Attempt to read the email with UTF-8 encoding\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    text = file.read()  # Read the content of the email\n",
        "                    processed_text = preprocess_text(text)  # Preprocess the email text\n",
        "                    test_emails.append(processed_text)  # Append processed text to the list\n",
        "            except UnicodeDecodeError:\n",
        "                # Handle potential encoding issues by trying a different encoding\n",
        "                with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
        "                    text = file.read()\n",
        "                    processed_text = preprocess_text(text)\n",
        "                    test_emails.append(processed_text)\n",
        "\n",
        "    # vectorize test emails(tfidf vectorizer is fitted and performaed transformation already)\n",
        "    test_emails_vectorized = vectorizer.transform(test_emails)\n",
        "\n",
        "    # Predict spam/non-spam labels\n",
        "    predictions = svm_model.predict(test_emails_vectorized)\n",
        "\n",
        "    # Save predictions to file\n",
        "    with open('predictions.csv', 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['Filename', 'Prediction'])  # Write header row\n",
        "        for i, prediction in enumerate(predictions):\n",
        "            filename = os.listdir(test_folder)[i]  # Get filename\n",
        "            writer.writerow([filename, prediction])\n",
        "\n",
        "svm_model = svm_rbf\n",
        "vectorizer = vectorizer\n",
        "# Calling the prediction function\n",
        "predict_test_emails(svm_model, vectorizer)"
      ],
      "metadata": {
        "id": "Euri5AMnyQBj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}