# -*- coding: utf-8 -*-
"""Assignment-3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mDoO-czZZUdKhjC7FRorFHN3WIU_JNDT
"""

import pandas as pd
import numpy as np

splits = {'train': 'mnist/train-00000-of-00001.parquet', 'test': 'mnist/test-00000-of-00001.parquet'}
df = pd.read_parquet("hf://datasets/ylecun/mnist/" + splits["train"])

print(df.iloc[0]['image'])  # Display the first few rows

# Get unique labels (classes)
unique_labels = df['label'].unique()
print(f'Unique labels (classes): {unique_labels}')

"""# To get an idea of the digit images, one image of each class will be displayed here."""

!pip install Pillow
from PIL import Image
import io

import matplotlib.pyplot as plt

# Iterate through unique labels and display one image for each
for label in unique_labels:
    # Filter DataFrame for the current label
    filtered_df = df[df['label'] == label]

    # Get the first image's byte data from the filtered DataFrame
    # Access the 'bytes' key to get the actual image bytes
    image_bytes = filtered_df['image'].iloc[0]['bytes']

    # Decode the byte data into an image
    image = Image.open(io.BytesIO(image_bytes))

    # Convert the image to a NumPy array
    image_np = np.array(image)

    # Print the image using Matplotlib
    plt.imshow(image_np, cmap='gray')  # Use 'gray' colormap for grayscale images
    plt.title(f"Class: {label}")
    plt.xlabel("Pixel Width")
    plt.ylabel("Pixel Height")
    plt.axis('on')
    plt.show()

data = [{'image': np.array(Image.open(io.BytesIO(x['bytes']))), 'label': label} for x, label in zip(df['image'], df['label'])]

data[0]

data[0]['image'].shape

"""# To write a function that picks a random set of 1000 images having 100 images from each class 0-9, to use as our dataset."""

import random

def sample_images(data, num_samples_per_class):
    sampled_data = []
    for label in range(10):  # Loop through each class 0-9
        class_samples = [x for x in data if x['label'] == label]
        sampled_data.extend(random.sample(class_samples, num_samples_per_class))
    return sampled_data

# Sample 100 images per class
sampled_data = sample_images(data, num_samples_per_class=10)

# Extract images and labels
sampled_images = np.array([x['image'].flatten() for x in sampled_data])  # Flatten 28x28 images
sampled_labels = np.array([x['label'] for x in sampled_data])

sampled_labels

sampled_images[0]

"""# Preprocessing the datset to store the pixel values separately and labels separately"""

def pca(data):
    mean = np.mean(data, axis=0)
    centered_data = data - mean

    no_samples = len(centered_data)
    no_features = len(centered_data[0])

    cov_matrix = np.dot(centered_data.T, centered_data) / (no_samples)

    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)

    sorted_indices = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[sorted_indices]
    eigenvectors = eigenvectors[:, sorted_indices]

    num_of_components = len(eigenvalues)
    return eigenvalues, eigenvectors, num_of_components

"""# Calling the PCA algorithm to act on our dataset"""

eigenvalues, eigenvectors, no_of_components = pca(sampled_images)

no_of_components

"""# Calculating the Variance explained by each of the principal components"""

variance_explained = eigenvalues / np.sum(eigenvalues)
cumulative_variance = np.cumsum(variance_explained)

"""# To study the components that together contribute towards 95% of the total variance"""

components_for_95_variance = np.argmax(cumulative_variance >= 0.95) + 1
print(f"Number of components contributing to 95% variance: {components_for_95_variance}")

"""# It means only one component is enough to capture the 95% of the variance (or the information in the data) for the given dataset.

# We will try to study the top 20 principal components
"""

num_components = 20
for i in range(num_components):
    print(f"PC {i+1}: {eigenvectors[i]} -> Variance :{variance_explained[i]:.4f}")

for i in range(num_components):
    print(f"PC {i+1}: Variance :{variance_explained[i]:.4f}")

"""# Visualizing Top 132(since there are 784 components) principal components as images"""

# Number of components to display
num_components_to_display = 132

# Reshape the eigenvectors (principal components) into 28x28 images
num_rows = 27  # Number of rows
num_cols = 5   # Number of columns

# Create a figure and axes using plt.subplots
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 15))

# Flatten axes array to iterate over it
axes = axes.flatten()

# Loop through the top 100 components and plot them as images
for i in range(num_components_to_display):
    pc_image = eigenvectors[:, i].reshape(28, 28)  # Reshape eigenvector into 28x28 image
    axes[i].imshow(pc_image, cmap='gray')
    axes[i].set_title(f"PC {i + 1}")
    axes[i].axis('off')  # Hide axis

# Hide empty subplots if there are any (e.g., if num_components_to_display < 100)
for j in range(num_components_to_display, len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.show()

"""# Variances explained by each of the principal components is shown in the form of a plot."""

# Plot the explained variance by each principal component
plt.figure(figsize=(10, 6))
plt.plot(variance_explained, marker='o', linestyle='-', color='b')
plt.title('Variance Explained by Each Principal Component')
plt.xlabel('Principal Component')
plt.ylabel('Variance Explained')
plt.grid(True)
plt.show()

"""# To get better idea, how principal components capture the image data as the number of components increases, here we plot the commulative variance plot"""

# Plot the cumulative variance explained
plt.figure(figsize=(10, 6))
plt.plot(np.cumsum(variance_explained), marker='o', linestyle='--', color='r')
plt.title('Cumulative Variance Explained')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Variance Explained')
plt.grid(True)
plt.show()

"""#(ii) Reconstruction of [link text](https:// [link text](https:// [link text](https://)))the datset using principal components that conttribute to the 95% variance, acting as different dimensional representations"""

num_components_95 = components_for_95_variance

top_eigenvectors = eigenvectors[:, :num_components_95]

centered_data = sampled_images - np.mean(sampled_images, axis=0)

top_eigenvectors = eigenvectors[:, :num_components_95]

reduced_data = np.dot(centered_data, top_eigenvectors)

reconstructed_data = np.dot(reduced_data, top_eigenvectors.T) + np.mean(sampled_images, axis=0)

# Number of samples to display
num_samples = sampled_images.shape[0]
num_samples_per_class = 10
num_classes = 10

fig, axes = plt.subplots(num_classes, num_samples_per_class, figsize=(20, 20))

# Loop through each class and display its images
for i, label in enumerate(range(num_classes)):
    # Filter the data for the current label
    class_data = [x['image'] for x in sampled_data if x['label'] == label]

    for j in range(num_samples_per_class):
        reconstructed_img = reconstructed_data[i * num_samples_per_class + j].reshape(28, 28)
        axes[i, j].imshow(reconstructed_img, cmap='gray')
        axes[i, j].set_title(f" {label}")
        axes[i, j].axis('off')

plt.tight_layout()
plt.show()

plt.tight_layout()
plt.show()

# Step 6: Evaluate the reconstruction quality
# Reconstructed data is stored in `reconstructed_data`. We can assess the quality of the reconstruction by measuring
# the mean squared error (MSE) between the original and reconstructed data.
mse = np.mean((sampled_images - reconstructed_data) ** 2)
print(f"Mean Squared Error (MSE) between original and reconstructed data: {mse:.4f}")



"""# Question2 - implementing lloyd's algorithm for k-means
part (i)
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.spatial.distance import cdist

file_path = '/content/cm_dataset_2.csv'
df = pd.read_csv(file_path, names=['x', 'y'])

df.shape



points = df.to_numpy()

from math import ceil
def kmeans(points, k, max_iters=100, random_state=None):
    """K-Means implementation using Lloyd's algorithm."""
    np.random.seed(random_state)
    centroids = points[np.random.choice(points.shape[0], k, replace=False)]
    errors = []

    for _ in range(max_iters):
        labels = pairwise_distances_argmin(points, centroids)
        error = np.sum((points - centroids[labels])**2)
        errors.append(error)
        new_centers = np.array([points[labels == i].mean(axis=0) for i in range(k)])

        if np.all(centroids == new_centers):
            break

        centroids = new_centers

    return centroids, labels, errors

# Lloyd's Algorithm for k=2
k = 2
initializations = 5
plt.figure(figsize=(15,10))

for i in range(initializations):
    centroids, labels, errors = kmeans(points, k, max_iters = 100, random_state=i)
    plt.subplot(2, initializations, i+1)
    plt.plot(errors, marker='o')
    plt.title(f'Initialization {i+1} - Error Function')
    plt.xlabel('Iteration')
    plt.ylabel('Error')
    plt.subplot(2, initializations, initializations+i+1)
    for cluster in range(k):
        cluster_points = points[labels == cluster]
        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster+1}')
    plt.scatter(centroids[:,0], centroids[:,1], c='black',marker='x',s=100, label='Centroids')
    plt.title(f'Initialization {i+1} - Clusters')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()

plt.tight_layout()
plt.show()

"""#(ii). For each K = {2, 3, 4, 5} using fixed arbitrary initialization, we need to obtain cluster centers according to K-means algorithm. Also, for each K, we need to plot voronoi regions associated to each cluster center. Note that we have assumed the minimum and maximum value in the data-set to be the range for each component in two dimensions."""

def kmeans_fixed_init(points, k, init_centroids, max_iters=100):
    """K-Means algorithm with fixed initialization."""
    centroids = np.array(init_centroids)  # Fixed initialization
    prev_centroids = np.zeros_like(centroids)
    labels = np.zeros(points.shape[0])

    for _ in range(max_iters):
        # Assign each point to the nearest centroid
        dist = cdist(points, centroids)
        labels = np.argmin(dist, axis=1)

        # Update centroids
        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])

        # Check for convergence (if centroids do not change)
        if np.all(centroids == new_centroids):
            break

        centroids = new_centroids

    return centroids, labels

def plot_voronoi_and_clusters(points, centroids, labels, k):
    """Plot Voronoi regions and clusters."""
    x_min, x_max = points[:, 0].min() - 1, points[:, 0].max() + 1
    y_min, y_max = points[:, 1].min() - 1, points[:, 1].max() + 1

    # Create a grid of points
    grid_x, grid_y = np.meshgrid(np.linspace(x_min, x_max, 500),
                                 np.linspace(y_min, y_max, 500))
    grid_points = np.c_[grid_x.ravel(), grid_y.ravel()]

    # Compute the distance to each centroid
    dist = cdist(grid_points, centroids)
    labels_grid = np.argmin(dist, axis=1)

    # Plot the Voronoi regions
    plt.figure(figsize=(10, 8))

    # Voronoi regions: Assign colors based on the closest centroid
    plt.imshow(labels_grid.reshape(grid_x.shape), origin='lower', cmap='viridis',
               extent=(x_min, x_max, y_min, y_max), alpha=0.4)

    # Plot the actual points
    plt.scatter(points[:, 0], points[:, 1], c=labels, cmap='viridis', s=30, label='Data points')

    # Plot the centroids
    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=100, label='Centroids')

    plt.title(f'Voronoi Diagram and Clusters (K={k})')
    plt.xlabel('X-axis')
    plt.ylabel('Y-axis')
    plt.legend()
    plt.show()

np.random.seed(42)  # Fixed seed for reproducibility
fixed_init_centroids = points[np.random.choice(points.shape[0], 5, replace=False)]

# Now for each value of K, we compute clusters and Voronoi regions
K_values = [2, 3, 4, 5]
for k in K_values:
    centroids, labels = kmeans_fixed_init(points, k, fixed_init_centroids[:k], max_iters=100)
    plot_voronoi_and_clusters(points, centroids, labels, k)